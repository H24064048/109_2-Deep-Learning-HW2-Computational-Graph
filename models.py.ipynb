{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "**Initialize Parameters** :  \n",
    "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "y = x\n",
    "y = 50\n",
    "x = y\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size:int, hidden_size:int, output_size:int, hidden_layer_act:str, output_layer_act:str, weight_init_std=.01):\n",
    "        # Initialize parameters\n",
    "        self.params = {}\n",
    "        self.params['w1'] = np.random.randn(input_size, hidden_size)/np.sqrt(input_size)\n",
    "        self.params['b1'] = np.zeros((hidden_size, 1))\n",
    "        self.params['w2'] = np.random.randn(hidden_size, output_size)/np.sqrt(hidden_size)\n",
    "        self.params['b2'] = np.random.randn((output_size, 1))\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['Linear1'] = Linear(self.params['w1'],self.params['b1'])\n",
    "        if hidden_layer_act == 'ReLU':\n",
    "            self.layers['ReLU_1'] = ReLU()\n",
    "        elif hidden_layer_act == 'Sigmoid':\n",
    "            self.layers['Sigmoid1'] = Sigmoid()\n",
    "        elif hidden_layer_act == 'tanh':\n",
    "            self.layers['tanh1'] = tanh()\n",
    "        elif hidden_layer_act == 'Leaky ReLU':\n",
    "            self.layers['LeakyReLu1'] = Leaky_ReLU()\n",
    "            \n",
    "        self.layers['Linear2'] = Linear(self.params['w2'],self.params['b2'])\n",
    "        self.output_layer = SoftmaxwithCrossEntropyLoss()\n",
    "        \n",
    "    def predict(self, x):  # x starts with data features and update through layers\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        yhat = self.predict(x)\n",
    "        return self.output_layer.forward(yhat, y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:  #book P137+ dimension reshape\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1) # (n_samples, 1d)\n",
    "        self.x = x\n",
    "        summ = np.dot(self.x, self.w) + self.b\n",
    "        return summ\n",
    "    \n",
    "    def backward(self, dout):   # need math handwrt\n",
    "        dx = np.dot(dprev, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dprev, axis=0)  # ?\n",
    "        dx = dx.reshape(*self.original_x_shape)  # ???\n",
    "        return dx\n",
    "    \n",
    "class ReLU: #book P128\n",
    "    def __init__(self, w, b):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "    \n",
    "class Sigmoid: #book P129\n",
    "    def __init__(self, w, b):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1/(1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "    \n",
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "x = np.array([[1,2,3],[3,4,5]])\n",
    "y = x.shape # (2,3)\n",
    "x = x.reshape(1,6)\n",
    "x.reshape(*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First argument : Hello\n",
      "Next argument through *argv : Welcome\n",
      "Next argument through *argv : to\n",
      "Next argument through *argv : GeeksforGeeks\n"
     ]
    }
   ],
   "source": [
    "# The special syntax *args in function definitions in python \n",
    "# is used to pass a variable number of arguments to a function. \n",
    "# It is used to pass a non-key worded, variable-length argument list. \n",
    "\n",
    "def myFun(arg1, *argv):\n",
    "    print (\"First argument :\", arg1)\n",
    "    for arg in argv:\n",
    "        print(\"Next argument through *argv :\", arg)\n",
    "  \n",
    "myFun('Hello', 'Welcome', 'to', 'GeeksforGeeks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxwithCrossEntropyLoss: #book P140\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.yhat = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.yhat = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.yhat, self.y)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        batch_size = self.y.shape[0]\n",
    "        if self.y.size == self.yhat.size :  #size1 ??\n",
    "            dx = (self.yhat - self.y) / batch_size   # 0/1 (pred prob - 1:real)\n",
    "        else:\n",
    "            dx = self.yhat.copy()\n",
    "            dx[np.arange(batch_size), self.y] -= 1\n",
    "            dx = dx / batch_size\n",
    "        return dx\n",
    "    \n",
    "def softmax(x):  #(n_samples, class_prob)\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.00305732e-02, 2.44728471e-01, 6.65240956e-01],\n",
       "       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],\n",
       "       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],\n",
       "       [2.91394551e-07, 4.74258594e-02, 9.52573849e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [9,10,11],\n",
    "              [3,15,18]])\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    print(np.max(x, axis=-1, keepdims=True))\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)\n",
    "\n",
    "def cross_entropy_error(y, yt):\n",
    "    if y.ndim == 1:\n",
    "        yt = yt.reshape(1, yt.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    if yt.size == y.size:\n",
    "        yt = yt.argmax(axis=1)     \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.mean(np.log(y[np.arange(batch_size), yt] + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [2.]\n",
      " [2.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.58797571],\n",
       "       [0.65900114],\n",
       "       [0.66524096]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= softmax([[1,2,0.9],[1,2,0.1],[1,2,0.0]])\n",
    "np.max(x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 這個function怪怪的\n",
    "\n",
    "def one_hot_transformation(y):\n",
    "    k = len(np.unique(y))\n",
    "    return np.eye(k)[y]\n",
    "\n",
    "np.shape(np.eye(5))\n",
    "np.eye(5)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "data count : 5\n",
      "(5, 3)\n",
      "data count : 5\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,0,0],\n",
    "              [0,1,0],\n",
    "              [0,0,1],\n",
    "              [0,0,1],\n",
    "              [1,0,0]])  # (5 data points, 3 cats)\n",
    "print(x.shape)\n",
    "print('data count :',x.shape[0])\n",
    "\n",
    "y = np.array([1,2,3,3,1])  # (5 data points, 3 cats)\n",
    "print(x.shape)\n",
    "print('data count :',x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans\n",
    "class SoftmaxWithCrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, yt):\n",
    "        self.yt = yt\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.yt)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout):\n",
    "        batch_size = self.yt.shape[0]\n",
    "        if self.yt.size == self.y.size: # deal with one-hot-encoding y\n",
    "            dx = (self.y - self.yt) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.yt] -= 1\n",
    "            dx = dx / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.92576723  1.05570681]\n",
      " [-1.3781495  -0.26838915]\n",
      " [ 0.9792404   0.31689814]\n",
      " [-0.52831287  1.53056277]]\n",
      "[1 2 3 4]\n",
      "[1 3 5 7]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(4,2)\n",
    "print(x)\n",
    "x= np.array([1,2,3,4])\n",
    "print(x)\n",
    "y = np.array([1,3,5,7])\n",
    "print(y)\n",
    "print(np.dot(x.T,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w \n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.x_shape = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):  # sum\n",
    "        self.x_shape = x.shape #?? reshape to (sample_size, list)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        summ = np.dot(self.x, self.w) + self.b\n",
    "        return summ\n",
    "    \n",
    "    def backward(self, dout):  # computational graph reverse method : d_output is known\n",
    "        dx = np.dot(dout, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1 11]\n",
      "  [12 13]]\n",
      "\n",
      " [[ 2 21]\n",
      "  [22 32]]\n",
      "\n",
      " [[ 3 13]\n",
      "  [23 33]]]\n",
      "(3, 2, 2)\n",
      "[[ 1 11 12 13]\n",
      " [ 2 21 22 32]\n",
      " [ 3 13 23 33]]\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# simple experiment\n",
    "x = np.array([1,11,12,13,2,21,22,32,3,13,23,33])\n",
    "x = x.reshape((3,2,2))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = x.reshape(x.shape[0], -1)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "odd = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd['1'] = 'a'\n",
    "odd['2'] = 'b'\n",
    "odd['3'] = 'c'\n",
    "odd['4'] = 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values(['a', 'b', 'c', 'd'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for od in odd.values() :\n",
    "    print(od)\n",
    "    \n",
    "for od in odd :\n",
    "    print(od)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
